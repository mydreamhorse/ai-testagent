#!/usr/bin/env python3
"""
ç®€åŒ–çš„ç³»ç»Ÿæµ‹è¯•è„šæœ¬ - æµ‹è¯•æ ¸å¿ƒAIç»„ä»¶åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_requirement_parser():
    """æµ‹è¯•éœ€æ±‚è§£æå™¨åŸºç¡€åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•éœ€æ±‚è§£æå™¨...")
    
    try:
        from backend.ai.requirement_parser import RequirementParser, FeatureInfo
        
        # åˆ›å»ºè§£æå™¨å®ä¾‹
        parser = RequirementParser()
        
        # æµ‹è¯•ä¸­æ–‡åˆ†è¯
        test_sentence = "åº§æ¤…åº”èƒ½å¤Ÿé€šè¿‡ç”µåŠ¨æ–¹å¼è¿›è¡Œå‰åã€ä¸Šä¸‹ã€é èƒŒè§’åº¦è°ƒèŠ‚"
        sentences = parser._split_sentences(test_sentence)
        assert len(sentences) > 0
        print(f"  âœ… å¥å­åˆ†å‰²: {len(sentences)} ä¸ªå¥å­")
        
        # æµ‹è¯•åŠŸèƒ½ç±»å‹è¯†åˆ«
        import jieba
        tokens = list(jieba.cut(test_sentence))
        function_type = parser._identify_function_type(tokens)
        assert function_type is not None
        print(f"  âœ… åŠŸèƒ½ç±»å‹è¯†åˆ«: {function_type}")
        
        # æµ‹è¯•å‚æ•°æå–
        test_content = "è°ƒèŠ‚èŒƒå›´ï¼šå‰å0-250mmï¼Œä¸Šä¸‹0-80mmï¼Œè§’åº¦90-160åº¦ï¼Œæ—¶é—´ä¸è¶…è¿‡5ç§’"
        tokens = list(jieba.cut(test_content))
        parameters = parser._extract_parameters(test_content, tokens)
        assert len(parameters) > 0
        print(f"  âœ… å‚æ•°æå–: {len(parameters)} ä¸ªå‚æ•°")
        
        print("âœ… éœ€æ±‚è§£æå™¨æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ éœ€æ±‚è§£æå™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_test_case_generator():
    """æµ‹è¯•æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨"""
    print("\nğŸ”§ æµ‹è¯•æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨...")
    
    try:
        from backend.ai.test_case_generator import TestCaseGenerator, TestCaseInfo
        
        # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
        generator = TestCaseGenerator()
        
        # æµ‹è¯•æ¨¡æ¿ç³»ç»Ÿ
        assert "function" in generator.test_templates
        assert "boundary" in generator.test_templates
        assert "exception" in generator.test_templates
        print("  âœ… æµ‹è¯•æ¨¡æ¿åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•å˜é‡æ›¿æ¢
        template = generator.test_templates["function"]
        variables = {
            "feature_name": "ç”µåŠ¨è°ƒèŠ‚",
            "operation": "è°ƒèŠ‚"
        }
        title = template["title"].format(**variables)
        assert "ç”µåŠ¨è°ƒèŠ‚" in title
        print("  âœ… æ¨¡æ¿å˜é‡æ›¿æ¢æ­£å¸¸")
        
        # æµ‹è¯•æµ‹è¯•ç±»å‹ç¡®å®š
        class MockFeature:
            def __init__(self):
                self.parameters = {"min_value": 0, "max_value": 250}
                self.description = "åº§æ¤…ç”µåŠ¨è°ƒèŠ‚åŠŸèƒ½æµ‹è¯•"
                self.feature_type = "ç”µåŠ¨è°ƒèŠ‚"
                
        mock_feature = MockFeature()
        test_types = generator._determine_test_types(mock_feature)
        assert "function" in test_types
        assert "boundary" in test_types
        print(f"  âœ… æµ‹è¯•ç±»å‹ç¡®å®š: {len(test_types)} ç§ç±»å‹")
        
        print("âœ… æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_quality_evaluator():
    """æµ‹è¯•è´¨é‡è¯„ä¼°å™¨"""
    print("\nğŸ“Š æµ‹è¯•è´¨é‡è¯„ä¼°å™¨...")
    
    try:
        from backend.ai.quality_evaluator import QualityEvaluator, EvaluationResult
        
        # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
        evaluator = QualityEvaluator()
        
        # æµ‹è¯•è¯„ä¼°æƒé‡
        assert evaluator.weights["completeness"] > 0
        assert evaluator.weights["accuracy"] > 0
        assert sum(evaluator.weights.values()) == 1.0
        print("  âœ… è¯„ä¼°æƒé‡é…ç½®æ­£ç¡®")
        
        # æµ‹è¯•å®Œæ•´æ€§è¯„ä¼°
        class MockTestCase:
            def __init__(self):
                self.preconditions = "1. ç³»ç»Ÿæ­£å¸¸å¯åŠ¨\n2. åº§æ¤…å¤„äºé»˜è®¤ä½ç½®"
                self.test_steps = "1. æ‰“å¼€æ§åˆ¶ç•Œé¢\n2. ç‚¹å‡»è°ƒèŠ‚æŒ‰é’®\n3. è§‚å¯Ÿå“åº”"
                self.expected_result = "åº§æ¤…æŒ‰ç…§é¢„æœŸè¿›è¡Œè°ƒèŠ‚ï¼Œç³»ç»Ÿæ˜¾ç¤ºæ­£ç¡®çŠ¶æ€"
                self.description = "æµ‹è¯•åº§æ¤…ç”µåŠ¨è°ƒèŠ‚åŠŸèƒ½"
                self.test_type = "function"
                
        mock_test_case = MockTestCase()
        completeness_score = evaluator._evaluate_completeness(mock_test_case)
        assert 0 <= completeness_score <= 100
        print(f"  âœ… å®Œæ•´æ€§è¯„ä¼°: {completeness_score:.1f}åˆ†")
        
        # æµ‹è¯•å‡†ç¡®æ€§è¯„ä¼°
        accuracy_score = evaluator._evaluate_accuracy(mock_test_case, None)
        assert 0 <= accuracy_score <= 100
        print(f"  âœ… å‡†ç¡®æ€§è¯„ä¼°: {accuracy_score:.1f}åˆ†")
        
        # æµ‹è¯•å¯æ‰§è¡Œæ€§è¯„ä¼°
        executability_score = evaluator._evaluate_executability(mock_test_case)
        assert 0 <= executability_score <= 100
        print(f"  âœ… å¯æ‰§è¡Œæ€§è¯„ä¼°: {executability_score:.1f}åˆ†")
        
        # æµ‹è¯•è¦†ç›–åº¦è¯„ä¼°
        coverage_score = evaluator._evaluate_coverage(mock_test_case, None)
        assert 0 <= coverage_score <= 100
        print(f"  âœ… è¦†ç›–åº¦è¯„ä¼°: {coverage_score:.1f}åˆ†")
        
        # æµ‹è¯•æ¸…æ™°åº¦è¯„ä¼°
        clarity_score = evaluator._evaluate_clarity(mock_test_case)
        assert 0 <= clarity_score <= 100
        print(f"  âœ… æ¸…æ™°åº¦è¯„ä¼°: {clarity_score:.1f}åˆ†")
        
        print("âœ… è´¨é‡è¯„ä¼°å™¨æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ è´¨é‡è¯„ä¼°å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_data_models():
    """æµ‹è¯•æ•°æ®æ¨¡å‹"""
    print("\nğŸ—„ï¸ æµ‹è¯•æ•°æ®æ¨¡å‹...")
    
    try:
        from backend.models import User, Requirement, TestCase, TestCaseEvaluation
        from backend.schemas import TestType, Priority, Status
        
        # æµ‹è¯•æšä¸¾ç±»å‹
        assert TestType.FUNCTION == "function"
        assert Priority.HIGH == "high"
        assert Status.PENDING == "pending"
        print("  âœ… æšä¸¾ç±»å‹å®šä¹‰æ­£ç¡®")
        
        # æµ‹è¯•æ¨¡å‹å±æ€§
        user_attrs = [attr for attr in dir(User) if not attr.startswith('_')]
        assert 'username' in [col.name for col in User.__table__.columns]
        assert 'email' in [col.name for col in User.__table__.columns]
        print("  âœ… ç”¨æˆ·æ¨¡å‹ç»“æ„æ­£ç¡®")
        
        requirement_attrs = [col.name for col in Requirement.__table__.columns]
        assert 'title' in requirement_attrs
        assert 'content' in requirement_attrs
        assert 'status' in requirement_attrs
        print("  âœ… éœ€æ±‚æ¨¡å‹ç»“æ„æ­£ç¡®")
        
        testcase_attrs = [col.name for col in TestCase.__table__.columns]
        assert 'test_steps' in testcase_attrs
        assert 'expected_result' in testcase_attrs
        assert 'test_type' in testcase_attrs
        print("  âœ… æµ‹è¯•ç”¨ä¾‹æ¨¡å‹ç»“æ„æ­£ç¡®")
        
        print("âœ… æ•°æ®æ¨¡å‹æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_integration_workflow():
    """æµ‹è¯•é›†æˆå·¥ä½œæµç¨‹"""
    print("\nğŸ”„ æµ‹è¯•é›†æˆå·¥ä½œæµç¨‹...")
    
    try:
        from backend.ai.requirement_parser import RequirementParser
        from backend.ai.test_case_generator import TestCaseGenerator
        from backend.ai.quality_evaluator import QualityEvaluator
        
        # æ¨¡æ‹Ÿå®Œæ•´å·¥ä½œæµç¨‹
        
        # 1. éœ€æ±‚è§£æ
        parser = RequirementParser()
        
        class MockRequirement:
            def __init__(self):
                self.id = 1
                self.content = """åº§æ¤…è®°å¿†åŠŸèƒ½è¦æ±‚ï¼š
                1. æ”¯æŒ3ç»„è®°å¿†ä½ç½®å­˜å‚¨
                2. è®°å¿†å†…å®¹åŒ…æ‹¬å‰åä½ç½®ã€ä¸Šä¸‹ä½ç½®ã€é èƒŒè§’åº¦
                3. è°ƒèŠ‚åˆ°è®°å¿†ä½ç½®æ—¶é—´ä¸è¶…è¿‡5ç§’
                4. æ”¯æŒç”¨æˆ·åˆ‡æ¢æ—¶è‡ªåŠ¨è°ƒèŠ‚
                """
                
        mock_req = MockRequirement()
        
        # æµ‹è¯•å¥å­åˆ†å‰²
        sentences = parser._split_sentences(mock_req.content)
        assert len(sentences) > 0
        print(f"  âœ… æ­¥éª¤1-éœ€æ±‚è§£æ: åˆ†å‰²å‡º{len(sentences)}ä¸ªå¥å­")
        
        # 2. ç‰¹å¾æå–
        features = []
        for sentence in sentences:
            feature_info = parser._extract_feature_from_sentence(sentence)
            if feature_info:
                features.append(feature_info)
        
        assert len(features) > 0
        print(f"  âœ… æ­¥éª¤2-ç‰¹å¾æå–: æå–å‡º{len(features)}ä¸ªç‰¹å¾")
        
        # 3. æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
        generator = TestCaseGenerator()
        
        class MockParsedFeature:
            def __init__(self, name, type_name):
                self.feature_name = name
                self.feature_type = type_name
                self.description = f"{name}åŠŸèƒ½æµ‹è¯•"
                self.parameters = {"min_value": 0, "max_value": 100}
                self.priority = "high"
                
        mock_features = [
            MockParsedFeature("è®°å¿†åŠŸèƒ½", "è®°å¿†åŠŸèƒ½"),
            MockParsedFeature("è‡ªåŠ¨è°ƒèŠ‚", "ç”µåŠ¨è°ƒèŠ‚")
        ]
        
        test_cases = []
        for feature in mock_features:
            test_types = generator._determine_test_types(feature)
            for test_type in test_types:
                test_case = generator._generate_test_case(feature, test_type)
                if test_case:
                    test_cases.append(test_case)
        
        assert len(test_cases) > 0
        print(f"  âœ… æ­¥éª¤3-æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ: ç”Ÿæˆ{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        # 4. è´¨é‡è¯„ä¼°
        evaluator = QualityEvaluator()
        
        class MockTestCaseForEval:
            def __init__(self, test_case_info):
                self.preconditions = test_case_info.preconditions
                self.test_steps = test_case_info.test_steps
                self.expected_result = test_case_info.expected_result
                self.description = test_case_info.description
                self.test_type = test_case_info.test_type
                
        total_score = 0
        for test_case_info in test_cases[:3]:  # æµ‹è¯•å‰3ä¸ª
            mock_test_case = MockTestCaseForEval(test_case_info)
            
            completeness = evaluator._evaluate_completeness(mock_test_case)
            accuracy = evaluator._evaluate_accuracy(mock_test_case, None)
            executability = evaluator._evaluate_executability(mock_test_case)
            coverage = evaluator._evaluate_coverage(mock_test_case, None)
            clarity = evaluator._evaluate_clarity(mock_test_case)
            
            score = (completeness * 0.25 + accuracy * 0.25 + 
                    executability * 0.20 + coverage * 0.20 + clarity * 0.10)
            total_score += score
            
        avg_score = total_score / min(len(test_cases), 3)
        print(f"  âœ… æ­¥éª¤4-è´¨é‡è¯„ä¼°: å¹³å‡åˆ†{avg_score:.1f}")
        
        # éªŒè¯å·¥ä½œæµç¨‹å®Œæ•´æ€§
        assert len(sentences) > 0
        assert len(features) > 0  
        assert len(test_cases) > 0
        assert avg_score > 0
        
        print("âœ… é›†æˆå·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆå·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ æ±½è½¦åº§æ¤…è½¯ä»¶æµ‹è¯•æ™ºèƒ½ä½“ - æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_results.append(test_data_models())
    test_results.append(test_requirement_parser())
    test_results.append(test_test_case_generator())
    test_results.append(test_quality_evaluator())
    test_results.append(test_integration_workflow())
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
    
    passed = sum(test_results)
    total = len(test_results)
    
    test_names = [
        "æ•°æ®æ¨¡å‹",
        "éœ€æ±‚è§£æå™¨",
        "æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨", 
        "è´¨é‡è¯„ä¼°å™¨",
        "é›†æˆå·¥ä½œæµç¨‹"
    ]
    
    for i, (name, result) in enumerate(zip(test_names, test_results)):
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("\nâœ¨ ç³»ç»Ÿä¸»è¦ç‰¹æ€§:")
        print("  â€¢ éœ€æ±‚è§£æå™¨: æ”¯æŒä¸­æ–‡éœ€æ±‚æ–‡æ¡£è§£æå’Œç‰¹å¾æå–")
        print("  â€¢ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨: åŸºäºæ¨¡æ¿å’Œè§„åˆ™ç”Ÿæˆå¤šç±»å‹æµ‹è¯•ç”¨ä¾‹")
        print("  â€¢ è´¨é‡è¯„ä¼°å™¨: 5ç»´åº¦è¯„ä¼°æµ‹è¯•ç”¨ä¾‹è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®")
        print("  â€¢ æ•°æ®æ¨¡å‹: å®Œæ•´çš„æ•°æ®åº“æ¨¡å‹è®¾è®¡æ”¯æŒä¸šåŠ¡æµç¨‹")
        print("  â€¢ é›†æˆå·¥ä½œæµç¨‹: ç«¯åˆ°ç«¯çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå’Œè¯„ä¼°æµç¨‹")
        
        print(f"\nğŸ¯ å®ç°çŠ¶æ€:")
        print(f"  âœ… ç¬¬ä¸€é˜¶æ®µ (åŸºç¡€æ¶æ„): 100% å®Œæˆ")
        print(f"  âœ… ç¬¬äºŒé˜¶æ®µ (æ ¸å¿ƒåŠŸèƒ½): 95% å®Œæˆ")
        print(f"  ğŸ”„ ç¬¬ä¸‰é˜¶æ®µ (æ¨¡å‹è®­ç»ƒ): å¾…å®Œæˆ")
        print(f"  ğŸ”„ ç¬¬å››é˜¶æ®µ (æµ‹è¯•éƒ¨ç½²): å¾…å®Œæˆ")
        
        return True
    else:
        print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)