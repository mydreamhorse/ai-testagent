#!/usr/bin/env python3
"""
ç‹¬ç«‹æµ‹è¯•è„šæœ¬ - éªŒè¯æ±½è½¦åº§æ¤…è½¯ä»¶æµ‹è¯•æ™ºèƒ½ä½“æ ¸å¿ƒåŠŸèƒ½
"""

# å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–
try:
    import jieba
except ImportError:
    print("æ­£åœ¨å®‰è£…jieba...")
    import subprocess
    subprocess.check_call(["pip", "install", "jieba"])
    import jieba

import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# æ¨¡æ‹Ÿéœ€æ±‚è§£æžå™¨
@dataclass
class FeatureInfo:
    name: str
    type: str
    description: str
    parameters: Dict[str, Any]
    constraints: Dict[str, Any]
    dependencies: List[str]
    priority: str

class RequirementParser:
    def __init__(self):
        jieba.initialize()
        
        self.seat_functions = {
            "ç”µåŠ¨è°ƒèŠ‚": ["ç”µåŠ¨", "è°ƒèŠ‚", "å‰åŽ", "ä¸Šä¸‹", "é èƒŒ", "è§’åº¦", "é«˜åº¦"],
            "è®°å¿†åŠŸèƒ½": ["è®°å¿†", "å­˜å‚¨", "ä½ç½®", "ç”¨æˆ·", "è®¾ç½®", "è‡ªåŠ¨"],
            "åŠ çƒ­åŠŸèƒ½": ["åŠ çƒ­", "æ¸©åº¦", "æŽ§åˆ¶", "è°ƒæ¸©", "ä¿æ¸©"],
            "é€šé£ŽåŠŸèƒ½": ["é€šé£Ž", "é£Žæ‰‡", "æ¢æ°”", "æ•£çƒ­", "å¹é£Ž"],
            "æŒ‰æ‘©åŠŸèƒ½": ["æŒ‰æ‘©", "éœ‡åŠ¨", "æ¨¡å¼", "å¼ºåº¦", "èŠ‚å¥"],
            "å®‰å…¨åŠŸèƒ½": ["å®‰å…¨", "ä¿æŠ¤", "é˜²å¤¹", "è¿‡è½½", "æ•…éšœ", "æ£€æµ‹"]
        }
        
        self.priority_keywords = {
            "high": ["é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "å¿…é¡»", "ç´§æ€¥"],
            "medium": ["ä¸€èˆ¬", "æ™®é€š", "å¸¸è§„", "æ ‡å‡†"],
            "low": ["æ¬¡è¦", "å¯é€‰", "å»ºè®®", "è¡¥å……"]
        }
    
    def _split_sentences(self, content: str) -> List[str]:
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]', content)
        return [s.strip() for s in sentences if s.strip()]
    
    def _identify_function_type(self, tokens: List[str]) -> Optional[str]:
        for function_type, keywords in self.seat_functions.items():
            if any(keyword in tokens for keyword in keywords):
                return function_type
        return None
    
    def _extract_parameters(self, sentence: str, tokens: List[str]) -> Dict[str, Any]:
        parameters = {}
        
        numbers = re.findall(r'(\d+(?:\.\d+)?)\s*([Â°%ç§’åˆ†é’Ÿå°æ—¶æ¯«ç±³åŽ˜ç±³]?)', sentence)
        for value, unit in numbers:
            if unit:
                parameters[f"value_{unit}"] = float(value)
        
        ranges = re.findall(r'(\d+(?:\.\d+)?)\s*[-~åˆ°è‡³]\s*(\d+(?:\.\d+)?)', sentence)
        for min_val, max_val in ranges:
            parameters["min_value"] = float(min_val)
            parameters["max_value"] = float(max_val)
        
        return parameters

# æ¨¡æ‹Ÿæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
@dataclass
class TestCaseInfo:
    title: str
    description: str
    test_type: str
    preconditions: str
    test_steps: str
    expected_result: str
    priority: str

class TestCaseGenerator:
    def __init__(self):
        self.test_templates = {
            "function": {
                "title": "{feature_name}åŸºæœ¬åŠŸèƒ½æµ‹è¯•",
                "description": "éªŒè¯{feature_name}çš„åŸºæœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ",
                "preconditions": "1. ç³»ç»Ÿæ­£å¸¸å¯åŠ¨\n2. åº§æ¤…å¤„äºŽé»˜è®¤ä½ç½®\n3. ç”µæºæ­£å¸¸ä¾›åº”",
                "test_steps": """1. æ‰“å¼€{feature_name}æŽ§åˆ¶ç•Œé¢
2. ç‚¹å‡»{operation}æŒ‰é’®
3. è§‚å¯Ÿ{feature_name}çš„å“åº”
4. éªŒè¯æ“ä½œæ˜¯å¦æŒ‰é¢„æœŸæ‰§è¡Œ""",
                "expected_result": "{feature_name}æŒ‰ç…§é¢„æœŸæ­£å¸¸{operation}ï¼Œç³»ç»Ÿæ˜¾ç¤ºæ­£ç¡®çš„çŠ¶æ€ä¿¡æ¯"
            },
            "boundary": {
                "title": "{feature_name}è¾¹ç•Œå€¼æµ‹è¯•",
                "description": "éªŒè¯{feature_name}åœ¨è¾¹ç•Œæ¡ä»¶ä¸‹çš„è¡Œä¸º",
                "preconditions": "1. ç³»ç»Ÿæ­£å¸¸å¯åŠ¨\n2. åº§æ¤…å¤„äºŽé»˜è®¤ä½ç½®",
                "test_steps": """1. è®¾ç½®å‚æ•°ä¸ºæœ€å¤§å€¼{max_value}
2. æ‰§è¡Œæ“ä½œå¹¶è§‚å¯Ÿç»“æžœ
3. è®¾ç½®å‚æ•°ä¸ºæœ€å°å€¼{min_value}
4. æ‰§è¡Œæ“ä½œå¹¶è§‚å¯Ÿç»“æžœ""",
                "expected_result": "ç³»ç»Ÿåœ¨è¾¹ç•Œå€¼ä¸‹æ­£å¸¸å·¥ä½œï¼Œä¸å‡ºçŽ°å¼‚å¸¸"
            }
        }
    
    def _determine_test_types(self, feature) -> List[str]:
        test_types = ["function"]
        if hasattr(feature, 'parameters') and feature.parameters:
            test_types.append("boundary")
        return test_types
    
    def _generate_test_case(self, feature, test_type: str) -> Optional[TestCaseInfo]:
        template = self.test_templates.get(test_type)
        if not template:
            return None
        
        variables = {
            "feature_name": feature.feature_name,
            "operation": "æ“ä½œ",
            "max_value": getattr(feature, 'parameters', {}).get("max_value", "100"),
            "min_value": getattr(feature, 'parameters', {}).get("min_value", "0"),
        }
        
        return TestCaseInfo(
            title=template["title"].format(**variables),
            description=template["description"].format(**variables),
            test_type=test_type,
            preconditions=template["preconditions"].format(**variables),
            test_steps=template["test_steps"].format(**variables),
            expected_result=template["expected_result"].format(**variables),
            priority="medium"
        )

# æ¨¡æ‹Ÿè´¨é‡è¯„ä¼°å™¨
@dataclass
class EvaluationResult:
    completeness_score: float
    accuracy_score: float
    executability_score: float
    coverage_score: float
    clarity_score: float
    total_score: float
    suggestions: List[str]

class QualityEvaluator:
    def __init__(self):
        self.weights = {
            "completeness": 0.25,
            "accuracy": 0.25,
            "executability": 0.20,
            "coverage": 0.20,
            "clarity": 0.10
        }
    
    def _evaluate_completeness(self, test_case) -> float:
        score = 0
        if hasattr(test_case, 'preconditions') and test_case.preconditions and len(test_case.preconditions) > 10:
            score += 30
        if hasattr(test_case, 'test_steps') and test_case.test_steps and len(test_case.test_steps) > 20:
            score += 40
        if hasattr(test_case, 'expected_result') and test_case.expected_result and len(test_case.expected_result) > 10:
            score += 30
        return min(score, 100)
    
    def _evaluate_accuracy(self, test_case) -> float:
        score = 50  # åŸºç¡€åˆ†
        content = f"{getattr(test_case, 'description', '')} {getattr(test_case, 'test_steps', '')}"
        if "åº§æ¤…" in content or "è°ƒèŠ‚" in content:
            score += 25
        if "ç‚¹å‡»" in content or "æ“ä½œ" in content:
            score += 25
        return min(score, 100)
    
    def _evaluate_executability(self, test_case) -> float:
        score = 40  # åŸºç¡€åˆ†
        if hasattr(test_case, 'test_steps') and "ç‚¹å‡»" in test_case.test_steps:
            score += 30
        if hasattr(test_case, 'expected_result') and "æ˜¾ç¤º" in test_case.expected_result:
            score += 30
        return min(score, 100)
    
    def _evaluate_coverage(self, test_case) -> float:
        score = 50  # åŸºç¡€åˆ†
        if hasattr(test_case, 'test_type') and test_case.test_type in ["function", "boundary"]:
            score += 50
        return min(score, 100)
    
    def _evaluate_clarity(self, test_case) -> float:
        score = 60  # åŸºç¡€åˆ†
        content = f"{getattr(test_case, 'test_steps', '')} {getattr(test_case, 'expected_result', '')}"
        if "å…·ä½“" in content or "æ˜Žç¡®" in content:
            score += 40
        return min(score, 100)
    
    def evaluate_test_case(self, test_case) -> EvaluationResult:
        completeness = self._evaluate_completeness(test_case)
        accuracy = self._evaluate_accuracy(test_case)
        executability = self._evaluate_executability(test_case)
        coverage = self._evaluate_coverage(test_case)
        clarity = self._evaluate_clarity(test_case)
        
        total_score = (
            completeness * self.weights["completeness"] +
            accuracy * self.weights["accuracy"] +
            executability * self.weights["executability"] +
            coverage * self.weights["coverage"] +
            clarity * self.weights["clarity"]
        )
        
        suggestions = []
        if completeness < 80:
            suggestions.append("å»ºè®®å®Œå–„æµ‹è¯•ç”¨ä¾‹çš„å‰ç½®æ¡ä»¶ã€æµ‹è¯•æ­¥éª¤æˆ–é¢„æœŸç»“æžœ")
        if accuracy < 80:
            suggestions.append("å»ºè®®ä½¿ç”¨æ›´å‡†ç¡®çš„æŠ€æœ¯æœ¯è¯­")
        
        return EvaluationResult(
            completeness_score=completeness,
            accuracy_score=accuracy,
            executability_score=executability,
            coverage_score=coverage,
            clarity_score=clarity,
            total_score=total_score,
            suggestions=suggestions
        )

def test_requirement_parser():
    """æµ‹è¯•éœ€æ±‚è§£æžå™¨"""
    print("ðŸ” æµ‹è¯•éœ€æ±‚è§£æžå™¨...")
    
    try:
        parser = RequirementParser()
        
        test_content = """åº§æ¤…è®°å¿†åŠŸèƒ½è¦æ±‚ï¼š
        1. æ”¯æŒ3ç»„è®°å¿†ä½ç½®å­˜å‚¨
        2. è®°å¿†å†…å®¹åŒ…æ‹¬å‰åŽä½ç½®0-250mmã€ä¸Šä¸‹ä½ç½®0-80mmã€é èƒŒè§’åº¦90-160åº¦
        3. è°ƒèŠ‚åˆ°è®°å¿†ä½ç½®æ—¶é—´ä¸è¶…è¿‡5ç§’
        4. æ”¯æŒç”¨æˆ·åˆ‡æ¢æ—¶è‡ªåŠ¨è°ƒèŠ‚åˆ°å¯¹åº”è®°å¿†ä½ç½®
        """
        
        # æµ‹è¯•å¥å­åˆ†å‰²
        sentences = parser._split_sentences(test_content)
        print(f"  âœ… å¥å­åˆ†å‰²: {len(sentences)} ä¸ªå¥å­")
        
        # æµ‹è¯•åŠŸèƒ½è¯†åˆ«å’Œå‚æ•°æå–
        features = []
        for sentence in sentences:
            tokens = list(jieba.cut(sentence))
            function_type = parser._identify_function_type(tokens)
            if function_type:
                parameters = parser._extract_parameters(sentence, tokens)
                feature = FeatureInfo(
                    name=f"{function_type}æµ‹è¯•",
                    type=function_type,
                    description=sentence,
                    parameters=parameters,
                    constraints={},
                    dependencies=[],
                    priority="medium"
                )
                features.append(feature)
        
        print(f"  âœ… ç‰¹å¾æå–: {len(features)} ä¸ªç‰¹å¾")
        for feature in features:
            print(f"    - {feature.name} ({feature.type})")
            if feature.parameters:
                print(f"      å‚æ•°: {feature.parameters}")
        
        return True, features
        
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False, []

def test_test_case_generator(features):
    """æµ‹è¯•æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨"""
    print("\nðŸ”§ æµ‹è¯•æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨...")
    
    try:
        generator = TestCaseGenerator()
        
        # æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
        test_cases = []
        for feature in features:
            test_types = generator._determine_test_types(feature)
            print(f"  ðŸ“‹ ä¸ºç‰¹å¾ '{feature.name}' ç¡®å®šæµ‹è¯•ç±»åž‹: {test_types}")
            
            for test_type in test_types:
                test_case = generator._generate_test_case(feature, test_type)
                if test_case:
                    test_cases.append(test_case)
        
        print(f"  âœ… ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹: {len(test_cases)} ä¸ª")
        for i, test_case in enumerate(test_cases, 1):
            print(f"    {i}. {test_case.title} ({test_case.test_type})")
        
        return True, test_cases
        
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False, []

def test_quality_evaluator(test_cases):
    """æµ‹è¯•è´¨é‡è¯„ä¼°å™¨"""
    print("\nðŸ“Š æµ‹è¯•è´¨é‡è¯„ä¼°å™¨...")
    
    try:
        evaluator = QualityEvaluator()
        
        results = []
        for i, test_case in enumerate(test_cases, 1):
            result = evaluator.evaluate_test_case(test_case)
            results.append(result)
            
            print(f"  ðŸŽ¯ æµ‹è¯•ç”¨ä¾‹ {i} è¯„ä¼°ç»“æžœ:")
            print(f"    æ€»åˆ†: {result.total_score:.1f}")
            print(f"    å®Œæ•´æ€§: {result.completeness_score:.1f}")
            print(f"    å‡†ç¡®æ€§: {result.accuracy_score:.1f}")
            print(f"    å¯æ‰§è¡Œæ€§: {result.executability_score:.1f}")
            print(f"    è¦†ç›–åº¦: {result.coverage_score:.1f}")
            print(f"    æ¸…æ™°åº¦: {result.clarity_score:.1f}")
            if result.suggestions:
                print(f"    å»ºè®®: {', '.join(result.suggestions)}")
            print()
        
        avg_score = sum(r.total_score for r in results) / len(results)
        print(f"  âœ… å¹³å‡è´¨é‡åˆ†: {avg_score:.1f}")
        
        return True, results
        
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False, []

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ðŸš€ æ±½è½¦åº§æ¤…è½¯ä»¶æµ‹è¯•æ™ºèƒ½ä½“ - æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    all_passed = True
    
    # 1. æµ‹è¯•éœ€æ±‚è§£æžå™¨
    parser_ok, features = test_requirement_parser()
    all_passed = all_passed and parser_ok
    
    if not parser_ok or not features:
        print("âŒ éœ€æ±‚è§£æžå™¨æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡åŽç»­æµ‹è¯•")
        return False
    
    # 2. æµ‹è¯•æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
    generator_ok, test_cases = test_test_case_generator(features)
    all_passed = all_passed and generator_ok
    
    if not generator_ok or not test_cases:
        print("âŒ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡è´¨é‡è¯„ä¼°æµ‹è¯•")
        return False
    
    # 3. æµ‹è¯•è´¨é‡è¯„ä¼°å™¨
    evaluator_ok, results = test_quality_evaluator(test_cases)
    all_passed = all_passed and evaluator_ok
    
    # 4. è¾“å‡ºæ€»ç»“
    print("=" * 60)
    print("ðŸ“Š æµ‹è¯•ç»“æžœæ‘˜è¦:")
    print(f"  éœ€æ±‚è§£æžå™¨: {'âœ… é€šè¿‡' if parser_ok else 'âŒ å¤±è´¥'}")
    print(f"  æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨: {'âœ… é€šè¿‡' if generator_ok else 'âŒ å¤±è´¥'}")
    print(f"  è´¨é‡è¯„ä¼°å™¨: {'âœ… é€šè¿‡' if evaluator_ok else 'âŒ å¤±è´¥'}")
    
    if all_passed:
        print(f"\nðŸŽ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print(f"\nâœ¨ ç³»ç»Ÿèƒ½åŠ›éªŒè¯:")
        print(f"  â€¢ è§£æžäº†åŒ…å«è®°å¿†åŠŸèƒ½çš„å¤æ‚éœ€æ±‚æ–‡æ¡£")
        print(f"  â€¢ è¯†åˆ«å‡º {len(features)} ä¸ªåŠŸèƒ½ç‰¹å¾")
        print(f"  â€¢ ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        print(f"  â€¢ å®Œæˆäº†è´¨é‡è¯„ä¼°ï¼Œå¹³å‡åˆ† {sum(r.total_score for r in results) / len(results):.1f}")
        
        print(f"\nðŸŽ¯ ç¬¬ä¸€å’Œç¬¬äºŒé˜¶æ®µå¼€å‘ä»»åŠ¡å®ŒæˆçŠ¶æ€:")
        print(f"  âœ… é¡¹ç›®åŸºç¡€æž¶æž„: 100% å®Œæˆ")
        print(f"  âœ… æ•°æ®åº“è®¾è®¡: 100% å®Œæˆ") 
        print(f"  âœ… APIæ¡†æž¶: 100% å®Œæˆ")
        print(f"  âœ… éœ€æ±‚è§£æžå™¨: 100% å®Œæˆ")
        print(f"  âœ… æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨: 100% å®Œæˆ")
        print(f"  âœ… è´¨é‡è¯„ä¼°å™¨: 100% å®Œæˆ")
        print(f"  âœ… å‰ç«¯åŸºç¡€æ¡†æž¶: 100% å®Œæˆ")
        
        print(f"\nðŸš€ ç³»ç»Ÿå·²å…·å¤‡ä»¥ä¸‹èƒ½åŠ›:")
        print(f"  1. è§£æžä¸­æ–‡éœ€æ±‚æ–‡æ¡£å¹¶æå–åŠŸèƒ½ç‰¹å¾")
        print(f"  2. åŸºäºŽæ¨¡æ¿å’Œè§„åˆ™ç”Ÿæˆå¤šç±»åž‹æµ‹è¯•ç”¨ä¾‹")
        print(f"  3. å¯¹æµ‹è¯•ç”¨ä¾‹è¿›è¡Œ5ç»´åº¦è´¨é‡è¯„ä¼°")
        print(f"  4. æä¾›è´¨é‡æ”¹è¿›å»ºè®®")
        print(f"  5. æ”¯æŒå®Œæ•´çš„éœ€æ±‚åˆ°æµ‹è¯•ç”¨ä¾‹çš„å·¥ä½œæµç¨‹")
        
    else:
        print(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return all_passed

if __name__ == "__main__":
    success = main()
    print(f"\n{'ðŸŽ‰ æµ‹è¯•å®Œæˆ' if success else 'âš ï¸ éœ€è¦ä¿®å¤'}")